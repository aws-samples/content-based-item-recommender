{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bace43-079d-4cb2-8420-1aa892af1fd8",
   "metadata": {},
   "source": [
    "# 02. Prompt Building and Inference test\n",
    "\n",
    "In this notebook we will test the inference. We will use LLM (Claude 2 on Amazon Bedrock) to convert a profile into suggested requirement. We then use text-to-embedding model (Amazon Titan Embedding on Bedrock) to find the suggested items which are closest to the requirement.\n",
    "\n",
    "An example:\n",
    "profile -> user bio or travel preference\n",
    "requirement -> suggested ideal places to travel to\n",
    "items -> the actual places that exist, which are the most relevant to the requirement.\n",
    "\n",
    "So overall, an example flow would be like the following. The application provides user travel profile. It asks LLM on what places might be good for the user to travel to. One/few-shot can be used here. The text-embedding model will then convert that into an embedding. A search will be conducted to find the most relevant actual places in the database which match the suggested place by the LLM. The place's text description will be returned.\n",
    "\n",
    "You can use this notebook to iteratively develop your prompt template. The next notebook will deploy the prompt template into the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950cb3f-90c4-4603-90ba-d1f690e56f74",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a6e4b-adbf-411b-81d6-e9331e1ee4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6be452-4d78-4ab7-8d17-f386ffbd4192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json, shutil, os, time, uuid\n",
    "import psycopg2, psycopg2.extras\n",
    "from helper.bastion import find_instances\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\")\n",
    "ssm = boto3.client(\"ssm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aed487-9b89-42f4-a1bf-14518b2bfd62",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Configure the database connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a17a4-1d5e-437a-89a5-67229a9737dd",
   "metadata": {},
   "source": [
    "Load the output variables from the CDK deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ae755-1202-44a2-82a1-3591d7d13f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment_output = json.load(open(\"./deployment-output.json\",\"r\"))\n",
    "rds_host = deployment_output[\"RecommenderStack\"][\"dbreaderendpoint\"]\n",
    "ssm_llm_parameter_name = deployment_output[\"RecommenderStack\"][\"ssmllmparametername\"]\n",
    "ssm_recommendation_parameter_name = deployment_output[\"RecommenderStack\"][\"ssmrecommendationparametername\"]\n",
    "bastion_asg = deployment_output[\"RecommenderStack\"][\"bastionhostasgname\"]\n",
    "bastion_id = find_instances(bastion_asg) if bastion_asg != \"\" else None\n",
    "connect_to_db_via_bastion = False # Set to True if you are running this Notebook without VPC connection to the DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea82ae-706f-42f7-a055-f32ce452bb42",
   "metadata": {},
   "source": [
    "Configure the Database class to interact with the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92274d13-430a-44ae-b4a9-84441899bcaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Database():\n",
    "    def __init__(self, reader, bastion_id=None, port=5432, database_name=\"vectordb\"):\n",
    "        self.reader_endpoint = reader\n",
    "        self.username = None\n",
    "        self.password = None\n",
    "        self.port = port\n",
    "        self.database_name = database_name\n",
    "        self.bastion_id = bastion_id # Also indicates that DB commands are run via a bastion host with AWS SSM.\n",
    "        self.conn = None\n",
    "    \n",
    "    def fetch_credentials(self):\n",
    "        secrets_manager = boto3.client(\"secretsmanager\")\n",
    "        credentials = json.loads(secrets_manager.get_secret_value(\n",
    "            SecretId='AuroraClusterCredentials'\n",
    "        )[\"SecretString\"])\n",
    "        self.username = credentials[\"username\"]\n",
    "        self.password = credentials[\"password\"]\n",
    "    \n",
    "    def connect_for_reading(self):\n",
    "        if self.username is None or self.password is None: self.fetch_credentials()\n",
    "        \n",
    "        conn = psycopg2.connect(host=self.reader_endpoint, port=self.port, user=self.username, password=self.password, database=self.database_name)\n",
    "        conn.autocommit = True\n",
    "        self.conn = conn\n",
    "        return conn\n",
    "    \n",
    "    def close_connection(self):\n",
    "        if self.conn is not None:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "    \n",
    "    def search(self, query_template, embedding, num_items=1, additional_query_parameters = []):\n",
    "        all_query_parameters = [embedding, str(num_items)] + additional_query_parameters\n",
    "        query_statement = query_template.format(*all_query_parameters)\n",
    "        return self.query_database(query_statement, tuples_only_and_unaligned=True, verbose=False)\n",
    "    \n",
    "    def query_database(self, query, tuples_only_and_unaligned=False, verbose=True):\n",
    "        if self.username is None or self.password is None: self.fetch_credentials()\n",
    "        \n",
    "        #print(query)\n",
    "        \n",
    "        if self.bastion_id is None or not connect_to_db_via_bastion:\n",
    "            if self.conn is None: self.connect_for_reading()\n",
    "            \n",
    "            cur = self.conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor)\n",
    "            cur.execute(query)\n",
    "            \n",
    "            try:\n",
    "                result = cur.fetchall()\n",
    "            except Exception as e:\n",
    "                if str(e) != \"no results to fetch\": print(e)\n",
    "                result = cur.statusmessage\n",
    "                \n",
    "            if verbose: print(result)\n",
    "            \n",
    "            cur.close()\n",
    "            return result\n",
    "            \n",
    "        else:\n",
    "            query_id = str(uuid.uuid4())[:8]\n",
    "            query_modifier = \" -At\" if tuples_only_and_unaligned  else \"\"\n",
    "                \n",
    "            query_command = f\"\"\"export PGPASSWORD='{self.password}' && echo \"{query}\" > ./q{query_id}.txt && psql -h {self.reader_endpoint} -p 5432 -U {self.username} -d {self.database_name} -F \"=@#@=\" -R \"===@###@===\" -f ./q{query_id}.txt {query_modifier} && rm ./q{query_id}.txt\"\"\"\n",
    "\n",
    "            response = ssm.send_command(\n",
    "                        InstanceIds=[self.bastion_id],\n",
    "                        DocumentName=\"AWS-RunShellScript\",\n",
    "                        Parameters={'commands': [query_command]})\n",
    "\n",
    "            command_id = response['Command']['CommandId']\n",
    "            flight_flag = True\n",
    "            while flight_flag:\n",
    "                try:\n",
    "                    output = ssm.get_command_invocation(\n",
    "                      CommandId=command_id,\n",
    "                      InstanceId=self.bastion_id\n",
    "                    )\n",
    "                    flight_flag = False\n",
    "\n",
    "                    if output['StandardOutputContent'] != '':\n",
    "                        records = list(map(lambda r: {\"id\": r.split(\"=@#@=\")[0], \"distance\":r.split(\"=@#@=\")[1], \"description\": r.split(\"=@#@=\")[2]} , output[\"StandardOutputContent\"].split(\"===@###@===\")))\n",
    "                        return records    \n",
    "                    \n",
    "                    output_string = \"\"\n",
    "                    if output[\"StandardOutputUrl\"] !=  '': output_string = output[\"StandardOutputUrl\"]\n",
    "                    if output[\"StandardErrorContent\"] !=  '': output_string = output[\"StandardErrorContent\"]\n",
    "                    if output[\"StandardErrorUrl\"] !=  '': output_string = output[\"StandardErrorUrl\"]\n",
    "\n",
    "                    if output[\"StandardErrorContent\"] !=  '' or output[\"StandardErrorUrl\"] !=  '':\n",
    "                        print(output_string)\n",
    "\n",
    "                    return output_string\n",
    "                except:\n",
    "                    time.sleep(1)\n",
    "            return output_string\n",
    "\n",
    "db = Database(reader=rds_host, bastion_id=bastion_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c82515-785b-46a4-ae35-a52cf6af67ee",
   "metadata": {},
   "source": [
    "### 3. Build the prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79806a9-d9a5-4273-a51c-808ac24dfe42",
   "metadata": {},
   "source": [
    "This is a prompt template to match with the mock dataset used. Feel free to change this one/few shot prompt example in accordance with your own dataset. Any value in {} are to be replaced during runtime. It is mandatory to have {0} to be the input text (the requirement/profile) and {1} to be the number of suggested item types to be requested. You can have more parameters, like {2}, {3}, and so on. During runtime, the AWS Lambda function will run `.format(*prompt_parameters)` on this prompt template. The `prompt_parameters` is a result of merge between `[input_text, num_types]` list and `additional_prompt_parameters` which defaults to `[]`. You can specify the `additional_prompt_parameters` in the API payload when doing the inference.\n",
    "\n",
    "Note that the `\\n\\nHuman:` and `\\n\\nAssistant:` structure needs to be preserved to make it work with the Claude 2 model on Amazon Bedrock. Also, ***use '###'*** to separate the suggested items since the AWS Lambda function is configured to split the results with this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee11d2-3751-4fe0-9584-ac632c8fbb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Human: You are a travel planner expert. Given a traveler profile, you can suggest best place types the person should visit in Singapore.\n",
    "See the below example.\n",
    "\n",
    "Suggest 2 place types to visit in Singapore for the below traveler profile.\n",
    "===Traveler Profile Example===\n",
    "Gender: Male\n",
    "Interest: Making family and friends happy. I am a backpacker who loves to go to new places, especially places with many people.\n",
    "I have gone to 76 countries and I love to try their local chocolates or biscuits. Sometimes I bring interesting local things for my colleagues.\n",
    "===2 Place Type Suggestion Example===\n",
    "###\n",
    "Place type: Shop\n",
    "Description: This shop sells very wide array of products including souverniers, chocolates, and biscuits, perfect for a treat for family with Singapore's unique items.\n",
    "###\n",
    "Place type: Traditional market\n",
    "Description: This place has many shops and merchants selling goodies from Singapore, from chocolates to cheap t-shirts. This also has some authentic atmosphere of the old Singapore, perfect for selfies.\n",
    "###\n",
    "\n",
    "Now given the example above, suggest {1} place types to visit in Singapore for the below traveler profile. Answer straight with the data WITHOUT added introduction sentence.\n",
    "===Traveler Profile===\n",
    "{0}\n",
    "==={1} Place Type Suggestion===\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "# Store it on disk\n",
    "path = \"prompt_template.txt\" # Do not change the naming of the file\n",
    "f = open(path, \"w\")\n",
    "f.write(prompt_template)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab18a0",
   "metadata": {},
   "source": [
    "### 4. Build the vector search query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e340bd5",
   "metadata": {},
   "source": [
    "Since this solution is customizable, it allows you to customer the vector search query. It will then be uploaded to S3 (in notebook 03) and be used by the Lambda function in the actual inference.\n",
    "\n",
    "Note that the AWS Lambda that backs the API is set to run `.format(*parameters)` from this template, while `parameters` will be a merged array of `[embedding, num_items]` and any additional parameters you supply during inference time. For example, if you want to add more parameters for the WHERE clause or other part of the query, you can do so by adding {2}, {3}, and so on. You must remember to supply these parameters via `additional_query_parameters` when invoking the inference API. By default the `additional_query_parameters` is and empty list `[]`.\n",
    "\n",
    "Another restriction is to always have the id and distance outputted and they must be the first and second column in the return result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a21b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_statement_template = \"SELECT id, embedding <-> '{0}' AS distance, description FROM items ORDER BY distance LIMIT {1};\"\n",
    "\n",
    "# Store it on disk\n",
    "path = \"vector_search_query.txt\" # Do not change the naming of the file\n",
    "f = open(path, \"w\")\n",
    "f.write(query_statement_template)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebd7ec-13c4-44f0-b8eb-45a100b19a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the value for additional query parameters (if any) now for testing\n",
    "additional_query_parameters = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75005e8c",
   "metadata": {},
   "source": [
    "### 5. Define parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbe182",
   "metadata": {},
   "source": [
    "Load the default parameters from the AWS SSM Parameter Store, then store them on file to be used by the next notebook. \n",
    "\n",
    "The LLM related parameters definition can be found here https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\n",
    "\n",
    "The recommendation related parameters are:\n",
    "* num_types = This is the number of the recommended item types to be returned by the LLM.\n",
    "* num_items = This is the number of items to be returned by the vectorDB for each vector being searched\n",
    "* model_id = This is the id of the model to be used in Amazon Bedrock. Please refer here https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html\n",
    "\n",
    "If you set `num_types=2` and `num_items=3`, this means that given a text input, you request LLM to recommended **2** item types. For each, this solution will convert them into embedding and do vector search to find the top **3** actual items in the database. So in total, you will have 2 x 3 = 6 items to be returned, assuming there is no duplication. This solution will do deduplication so the actual items to be returned can be less than num_types x num_items\n",
    "\n",
    "Note that `num_items` and `num_types` set on the recommendation parameters are just use as default values. They can be overridden during runtime if you specifcy those parameters in the API payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66301b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the LLM related parameters\n",
    "llm_parameters = json.loads(ssm.get_parameter(\n",
    "    Name=ssm_llm_parameter_name\n",
    ")['Parameter']['Value'])\n",
    "print(\"The LLM parameters are:\")\n",
    "llm_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b893961-5f83-4f59-8bac-acb2bf86635c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the recommendation related parameters\n",
    "recommendation_parameters = json.loads(ssm.get_parameter(\n",
    "    Name=ssm_recommendation_parameter_name\n",
    ")['Parameter']['Value'])\n",
    "print(\"The recommendation related parameters are:\")\n",
    "recommendation_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd44d43-533b-46f7-bf36-f94634b10e32",
   "metadata": {},
   "source": [
    "### 6. Test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749e510-f6e1-4240-a5bc-72be11992228",
   "metadata": {
    "tags": []
   },
   "source": [
    "Define a test profile.\n",
    "This test data is meant to be used with the mock dataset used in this solution. Feel free to change as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ca66e-11c8-43e1-8458-4c9fc734e2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_input=\"\"\"Female, 30 years old, married\n",
    "I really want to celebrate my wedding anniversary with Husband. \n",
    "Somewhere where we can have a picnic with our own bento with wide view of sky and city.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da771e-917c-4811-ad70-32c1b65069cc",
   "metadata": {},
   "source": [
    "Applying prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411adbd-f5a5-437a-b9ce-40c2a3108f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "additional_prompt_parameters = []\n",
    "prompt_parameters = [new_input, recommendation_parameters['num_types']] + additional_prompt_parameters\n",
    "prompt = prompt_template.format(*prompt_parameters)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d1c78-aaa4-4d2d-b063-42e4593ee2cf",
   "metadata": {},
   "source": [
    "Define inference test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee395795-052d-4c12-89f2-99bea300e297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_inference(prompt, num_items):\n",
    "    # Get the recommended item text from LLM\n",
    "    llm_parameters['prompt'] = prompt\n",
    "    body = json.dumps(llm_parameters)\n",
    "\n",
    "    # Call the LLM to get the suggested item types\n",
    "    response = bedrock.invoke_model(body=body, modelId=recommendation_parameters['model_id'])\n",
    "    recommended_item_types = json.loads(response.get(\"body\").read())[\"completion\"]\n",
    "    recommended_item_types = recommended_item_types.split(\"###\") if \"\\n###\" in recommended_item_types else [recommended_item_types]\n",
    "    recommended_item_types = list(filter(lambda x: x != '' and not x.isspace(), recommended_item_types))\n",
    "    \n",
    "    print(\"\\nLLM suggested item types:\")\n",
    "    print(recommended_item_types)\n",
    "\n",
    "    recommended_item_embeddings = []\n",
    "\n",
    "    # Call the text-to-embedding model to get the embedding for each of the suggested item types.\n",
    "    for item_type in recommended_item_types:\n",
    "        # Get the embedding of the recommended item text\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"inputText\": item_type,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = bedrock.invoke_model(body=body, modelId=\"amazon.titan-embed-text-v1\")\n",
    "        recommended_item_embeddings.append(json.loads(response.get(\"body\").read())[\"embedding\"])\n",
    "\n",
    "    recommended_items = []\n",
    "\n",
    "    # Do search on vector database\n",
    "    for embedding in recommended_item_embeddings:\n",
    "        recommended_items = recommended_items + db.search(query_statement_template, \n",
    "                                      embedding, \n",
    "                                      num_items=recommendation_parameters['num_items'])\n",
    "\n",
    "    # Deduplicate\n",
    "    final_recommended_items = {}\n",
    "    for item in sorted(recommended_items, key = lambda k: k[\"distance\"]):\n",
    "        if item['id'] not in final_recommended_items: final_recommended_items[item['id']] = item\n",
    "\n",
    "    final_recommended_items = list({'id': v[1]['id'], 'distance': v[1]['distance'], 'description': v[1]['description']} for v in final_recommended_items.items())\n",
    "    return final_recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566d967-957a-4e5a-a562-8b48e878243e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Do inference test with `num_items` = 1 and `num_types` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5ad4e-2caa-4538-bea1-05910f36aed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendation_parameters['num_items'] = 1\n",
    "test_inference(prompt, recommendation_parameters['num_items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b3967-157e-4fd8-a5ab-c1f93d779640",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's try with `num_items` = 2 and `num_types` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15ac63-47a8-4986-9ece-019f2d132c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendation_parameters['num_items'] = 2\n",
    "test_inference(prompt, recommendation_parameters['num_items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9582f786-b80f-446c-9c80-2e040d51abc7",
   "metadata": {},
   "source": [
    "Now let's try with `num_items` = 2 and `num_types` = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a9843-50cc-40c2-aca4-48007a79c8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendation_parameters['num_types'] = 2\n",
    "prompt_parameters = [new_input, recommendation_parameters['num_types'] ]\n",
    "prompt = prompt_template.format(*prompt_parameters)\n",
    "\n",
    "test_inference(prompt, recommendation_parameters['num_items'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a282dcd-1823-40b4-8670-22f2ab8fc990",
   "metadata": {},
   "source": [
    "If all is good, now let's verify and save the parameters to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75fd09-5205-4364-b2ce-eea96d68f35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Recommendation parameters are:\")\n",
    "recommendation_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744753db-a73f-42ab-b581-0768a2544d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del llm_parameters['prompt']\n",
    "print(\"LLM parameters are:\")\n",
    "llm_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee68cd1-84b7-429c-a299-0145cdd9f26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store it on disk\n",
    "path = \"llm_parameters.txt\" # Do not change the naming of the file\n",
    "f = open(path, \"w\")\n",
    "f.write(json.dumps(llm_parameters))\n",
    "f.close()\n",
    "\n",
    "# Store it on disk\n",
    "path = \"recommendation_parameters.txt\" # Do not change the naming of the file\n",
    "f = open(path, \"w\")\n",
    "f.write(json.dumps(recommendation_parameters))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
